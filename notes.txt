Goal: Train agent in ~15 minutes

Experiment 1:
Try to train the agent using a variational autoencoder
o How to train?
    - Training the autoencoder along-side the policy yielded results similar to just training the policy by itself
    - Training them in alternate phases was unsucecssfull. Perhaps the effect of having a "moving goal post" so to speak?
    - Pretraining the autoencoder seems most stable. [TODO]: Is it better than having no autoencoder?
o Architecture:
    - I tried a 3x2-layer MLP for the encoder/decoder and the CNN from the learning to drive in a day paper.
      Results? They had equal reconstruction loss, however, the MLP was faster to train.
      Although I suspect that the CNN might work better for input of higher dimentionality, and
      input that are real-life images.
    - MSE vs BCE: Turns out MSE has a lot lower reconstruction loss, however, the reconstructions themselves
      are of lower quality. Because of this I decided to stick to BCE for now.
      [TODO]: Try to use the MSE model with PPO
    - I tried several values of beta. It seems that it is able to enforce that the learned representations
      encode more relevant features, however, these representations arent very clear.
      I decided to stick to a value of beta=4 for now. [TODO]: Try beta=1

Experiment 2:
Try single environment
o I was able to train an agent using the PPO-VAE implementation with a single environment.
  However, I have not tested whether the VAE is actually necessary for this to work

Experiment 3:
Different reward formulations
o In this experiment, I want ot see if the reward formulation has a big impact on training time:
    - Reward formulation 1: The default CarRacing-v0 reward
        i.   -0.1 per step
        ii.  +1000/trackLenght for each visited tile
    - Reward formulation 2: The Kendall formulation
        i.   Reward = speed
        ii.  End episode upon infraction (e.g. driving of the road)
o Results:
    - Changing the environment such that driving of the road resets the episode VASTLY improves training time
    - reward1 vs reward2: reward2 seems sligthly better, although further investigation is needed [TODO]

[TDOD] Experiment 4:
Hyperparameter search
o Find the best hyperparameters

[TODO] Experiment 5:
Prioritized experience replay
o Make samples that had a higher value error more likely to be trained on (slightly different from TD-error)

[TODO] Experiment 6:
Try entropy-regularized RL
o Basically: add entropy term to reward to encourage high-entropy actions (see medium article for more info)

[TODO] Experiment 7:
Control behaviour
o Try to make the agent commit to actions for longer periods of time
    - Idea: Reduce training time by removing the high frequency oscilations at beginning of training
    - Possibly make "commitment time" a function of entropy? Basically, if the agent is unsure about a particular action
      make it commit to it for a longer time so that it can observe the results properly.
      

This env is harder than donkey as it is randomized


-- Hyperparam experiments --

- Using mse_cnn_zdim64_beta1_data10k VAE
- Using initial_std=0.4
- Using hidden vf and pi (500, 300)
- Using reward:
  def reward1(state):
      # -10 for driving off-road
      if state.off_road == True: return -10 * 0.1
      # + 1 x throttle
      reward = state.velocity * 0.001
      #reward -= 0.01
      return reward

Mujoco vs. Atari: Atari vastly superior

Trying to improve Atari:
1) lr=1e-4: "solved" env in 20 episodes
   Where solve is defined as getting score > 800 in eval
   800 points means the track was completed in 200/3=66.67s (-3 points per second, 1000 points in total)
   Also note that eval is computed every 5th, so could be solved before
2) epsilon=0.2: Better than 0.1
3) lr_decay=0.9: Better at the start, but detrimental later on. Decay too high?
4) lr_decay=0.98: Much better; does not beak the policy later on

Can we make the atari params work in an infinite horizon manner?

Setting T=inf

1) lr=2.5e-4: This learning rate is too high, although the paper is using a decaying learning rate, so this may be the result of the difference.
2) lr=1e-4:   Setting this as the initial learning rate helps a lot in this scenario also, however, a decaying learning rate seems to be necessary as well
3) lr=1e-4 && num_epochs=10: Increasing number of epochs does not seem to help
4) epsilon=0.2: ???

best runs so far: atari_params_lr0.0001_lr_decay0.98_epsilon0.2_horizon_inf and atari_params_lr0.0001_epsilon0.2, solved in 15 episodes
  Note that I believe this environment is a fair bit more difficult than the Wayve example, suggesting that PPO might be even stronger than DDPG for this task
  Also note that I tried DDPG, and couldnt get it to learn faster than ~50 episodes (and it was more unstable and erratic in its driving)

Overall, setting epsilon to 0.2 seems better than 0.1 and learning rate of 1e-4 is much better than anything greater.
Learning rate decay optional but a value of 0.98 seems overall better.
In general they all seem to learn a good policy within 20 episodes as long as the initial learning rate is about 1e-4.

The infinite horizon appears to make learning slower, but some experiments, such as atari_params_lr0.0001_lr_decay0.98_epsilon0.2_horizon_inf
shows to reach a good policy equaly fast as the finithe horizon scenario.
(Actually, both lower learning rate AND epsilon 0.2 seems to be necessary to have learning on the same line as non-finite according to the experiments)

-- Effect of choice of VAE --

We know mse_cnn_zdim64_beta1_data10k works well from before.

Trying bce_cnn_zdim64_beta1_data10k

Trying XXX_cnn_zdim64_beta4_data10k

Trying XXX_cnn_zdim10_betaX_data10k

Note which of these VAEs have the smallest reconstruction loss

-- State representation --

We know VAE works well.

1) VAE + measurements
2) VAE (+ measuremets) stacked
3) World Model

-- Action representation --

We know that the modified action function works well.
In this scheme, action[1] = speed is scaled such that =0 is medium throttle, =-1 is no throttle and 1 is full throttle

1) Have action[1] < 0 be breaking
2) Have action[2] for breaking
3) Have action[2] be boolean for breaking

-- Determine the effect of early terminiation --

Does early terminiation help with reducing training time?

-- Reward func --

We know that
r(a) = {
  -1,         if off_road
   v * 0.001, otherwise
}
works well.

1) How does scale affect learning? Multiply reward r(a) by some constant
2) Use a[1] instead of v
3) Simple +1 or v if alive
4) Use the "complicated" r(a) from Medium article
5) Try RDN reward only and in addition











